{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\mysorol\\바탕 화면\\imworking\\chkchk\\contents_based\\final_book3_token.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        본원 rd 정보 센터 에서는 글로벌 거대 기업 들 약진 거세지다 생 성형 인공 지능...\n",
       "1        < num > 대박 날 바이오 다크호스 와 함께 미지 길 성공 길 로 < num >...\n",
       "2        마케팅 으로 세상 바꾸다 최고 브랜드 되다 법 마케팅 전략 과 사례 로 배우다 마케...\n",
       "3        저 성장 인구 감소 시 대 어떻 게 소비자 설득 하고 새롭다 시장 만들다 것 인가 ...\n",
       "4        특수 물건 부동산 경매 통해 크다 수익 올리다 싶다 독자 들 위해 다양하다 성공사례...\n",
       "                               ...                        \n",
       "56927    상품 yes < num > 에서 구성 한 상품 이다 낱개 반품 불가 도서 연봉 말고...\n",
       "56928    지금 중국 주식 천만원 면 < num > 년 후 강남 아파트 산다 지금 현재 중국 ...\n",
       "56929    직장인 위 한 조직 생활 교과서 조직 사회 사회인 라면 누구 나 번은 거치다 인생 ...\n",
       "56930    상품 yes < num > 에서 구성 한 상품 이다 낱개 반품 불가 도서 충전 수업...\n",
       "56931    트렌드 코리아 < num > 양 해 소비 트렌드 키우다 들다 count sheep ...\n",
       "Name: tokenize_intro, Length: 56932, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = df['tokenize_intro']\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = df['tokenize_intro']\n",
    "d1.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 'full_name' 칼럼에 대해 TF-IDF 계산\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(d1)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# TF-IDF 결과를 데이터프레임으로 변환\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\ocr\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\ocr\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\ocr\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\ocr\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\ocr\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m decoder(doc)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\ocr\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# '책 소개글' 칼럼 추출\n",
    "d1 = df['intro']\n",
    "\n",
    "# TF-IDF 벡터화 객체 생성\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 'full_name' 칼럼에 대해 TF-IDF 계산\n",
    "tfidf_matrix = vectorizer.fit_transform(d1)\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# # 기존 데이터프레임에 TF-IDF 결과를 추가하거나 새로운 데이터프레임 생성\n",
    "# df_tfidf = pd.concat([df_INNER_JOIN, tfidf_df], axis=1)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(df_tfidf)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas에서 결측값을 나타내는 값은 null대신 missing 이라는 용어를 사용. missing data는 NaN 으로 표기한다. \n",
    " - 확인 방법 : .isnull().sum() , notnull() \n",
    " - 데이터 프레임 : df.isnull().sum().sum()\n",
    " - 결측값 제거 : d1 = df['intro'].dropna()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '책 소개글' 칼럼에서 NaN 값 제거\n",
    "d1 = df['intro'].dropna()\n",
    "\n",
    "# TF-IDF 벡터화 객체 생성\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 'full_name' 칼럼에 대해 TF-IDF 계산\n",
    "tfidf_matrix = vectorizer.fit_transform(d1)\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 인덱스 설정: 원래 데이터프레임의 인덱스를 유지\n",
    "tfidf_df.index = df['intro'].dropna().index\n",
    "\n",
    "# 결과 출력\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 활용 TF-IDF \n",
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intro를 데이터프레임에서 리스트로 변환\n",
    "intro_list = df['intro'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 intro를 공백으로 분리하여 words 리스트 생성\n",
    "words = []\n",
    "for intro in intro_list:\n",
    "    try:\n",
    "        words.append(intro.split())  # 각 intro을 분리하여 추가\n",
    "    except:\n",
    "        print(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4226"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 만들기\n",
    "id2word = corpora.Dictionary(words)  # fit dictionary\n",
    "\n",
    "# BoW 변환\n",
    "corpus = [id2word.doc2bow(word) for word in words]  # convert corpus to BoW format\n",
    "\n",
    "# TF-IDF 모델 학습\n",
    "tfidf = TfidfModel(corpus)  # fit model\n",
    "corpus_tfidf = tfidf[corpus]  # apply model to the first corpus document\n",
    "\n",
    "# 단어 사전에서 단어 ID를 단어로 변환\n",
    "terms = [id2word[i] for i in range(len(id2word))]\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame([[0] * len(terms) for _ in range(len(corpus))], columns=terms)\n",
    "\n",
    "# 각 문서에 대해 TF-IDF 값을 데이터프레임에 삽입\n",
    "for i, doc in enumerate(corpus_tfidf):\n",
    "    for term_id, value in doc:\n",
    "        tfidf_df.iloc[i, term_id] = value\n",
    "\n",
    "# 결과 출력\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 문서를 단어 토큰화 -> WORD2IDX 단어별로 인덱스 부여 (unique) \n",
    "문서 하나에 단어가 몇개 있는지 카운팅 \n",
    "word2idx로 인덱스랑 단어개수를 같이 저장 \n",
    "\n",
    "[\n",
    "문서1: [(1, 2), (4, 3), ...],\n",
    "]\n",
    "\n",
    "에 word2idx 카운팅 행렬을 만든다 \n",
    "\n",
    "문서 별 term frequency\n",
    "단어 별 document frequency (word2idx) -> 단어가 모든 문서 중에서 몇 개 문서에서 등장했는지 체크\n",
    "\n",
    "TF * IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4228 entries, 0 to 4227\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  4228 non-null   int64 \n",
      " 1   title         4228 non-null   object\n",
      " 2   full_name     4228 non-null   object\n",
      " 3   book_url      4228 non-null   object\n",
      " 4   authors       4228 non-null   object\n",
      " 5   publisher     4223 non-null   object\n",
      " 6   Unnamed: 0    4228 non-null   int64 \n",
      " 7   author_list   4228 non-null   object\n",
      " 8   item_infos    4228 non-null   object\n",
      " 9   cates         4228 non-null   object\n",
      " 10  intro         4226 non-null   object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 363.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# 전처리 \n",
    "df = pd.read_csv('inner_join_sample.csv')\n",
    "df.info()   # intro 칼럼의 타입 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['intro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 연속된 공백을 하나의 공백으로 치환\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 영어는 소문자로 변환\n",
    "    text = text.lower()\n",
    "    # 영어, 한글, 숫자, 공백을 제외한 모든 특수문자 제거\n",
    "    text = re.sub(r'[^a-z가-힣0-9\\s]', '', text)\n",
    "\n",
    "    # 형태소 분석 및 토큰화\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text , stem=True)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_recommendations(book_title, df, tfidf_matrix):\n",
    "    idx = df[df['title'] == book_title].index[0]\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
    "    similar_books_idx = cosine_sim.argsort()[-11:-1][::-1]\n",
    "    return df.iloc[similar_books_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner_join_sample.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintro\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mintro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# 연속된 공백을 하나의 공백으로 치환\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# 영어는 소문자로 변환\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\Users\\mysorol\\anaconda3\\envs\\nlp\\lib\\re.py:210\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('inner_join_sample.csv')\n",
    "df['intro'] = df['intro'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15년간 유머를 경영 리더십에 도입하여 80만 명을 웃기고 울린 유머 전도사 임붕영...\n",
      "1    마케팅 전문 매거북magabook을 출간하는 모라비안유니타스가 유니타스브랜드 vol...\n",
      "2    애기애타즉 자신을 사랑하고 자신을 사랑하듯 남과 조직을 사랑한다는 것이 바로 도산 ...\n",
      "3    최근 삼국지를 통해서 경영을 바라보려는 시도가 끊임없이 있어왔다ceo의 삼국지는 이...\n",
      "4    경제의 맥락을 짚어주고 경영의 지혜를 알려주는 좋은 책을 찾아낼 수 있도록 돕는 책...\n",
      "Name: processed_intro, dtype: object\n",
      "(4228,)\n"
     ]
    }
   ],
   "source": [
    "# TypeError: expected string or bytes-like object :re.sub 정규표현식 쓸때 text가 str아니라 발생 \n",
    "# str(text) 해주면 해결 \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text))  # 연속된 공백을 하나의 공백으로 치환\n",
    "    text = text.lower()  # 영어는 소문자로 변환\n",
    "    text = re.sub(r'[^a-z가-힣0-9\\s]', '', str(text))  # 영어, 한글, 숫자, 공백을 제외한 모든 특수문자 제거\n",
    "    return text\n",
    "\n",
    "# 전체 intro 열에 전처리 함수 적용\n",
    "df['processed_intro'] = df['intro'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 결과 확인\n",
    "print(df['processed_intro'].head())  # 첫 5개 행 출력\n",
    "print(df['processed_intro'].shape)    # 전체 행 수 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 및 토큰화\n",
    "def tokenize_text(text):\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text , stem=True)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15년 간 유머 경영 리더십 도입 80만 명 웃기 고 울리다 유머 전도사 임붕영 교...\n",
      "1    마케팅 전문 매 거북 magabook 추다 가다 모라비안 유니 타 스 유니 타 스 ...\n",
      "2    애기 애 타 즉 자신 사랑 하고 자신 사랑 하 듯 남 과 조직 사랑 한다는 것 바로...\n",
      "3    최근 삼국지 통해 서 경영 바라보다 시도 끊임없다 있다 오다 ceo 삼국지 이러하다...\n",
      "4    경제 맥락 짚다 경영 지혜 알다 좋다 책 찾아내다 수 있다 돕다 책 이다 책 학부 ...\n",
      "Name: tokenize_intro, dtype: object\n",
      "(4228,)\n"
     ]
    }
   ],
   "source": [
    "# 전체 intro 열에 전처리 함수 적용\n",
    "df['tokenize_intro'] = df['processed_intro'].apply(tokenize_text)\n",
    "\n",
    "# 전처리된 결과 확인\n",
    "print(df['tokenize_intro'].head())  # 첫 5개 행 출력\n",
    "print(df['tokenize_intro'].shape)    # 전체 행 수 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf로 벡터화 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4228, 16438)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 행렬의 형태 확인\n",
    "print(tfidf_matrix.shape)  # (문서 수, 단어 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.06650684 0.         0.         0.\n",
      "  0.06650684 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Dense 배열로 변환하여 일부 데이터 확인\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "print(dense_matrix[195:200, :20])  # 첫 5개 문서의 첫 10개 단어에 대한 TF-IDF 값 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '01', '01일', ..., '힘일', '힘주다', '힘차다'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 목록 확인\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5421"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = vectorizer.vocabulary_.get('리더십')\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'리더십'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[5421]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09385165]\n",
      " [0.16373011]\n",
      " [0.206661  ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 특정 단어의 TF-IDF 값 확인\n",
    "word_index = vectorizer.vocabulary_.get('리더십') # 단어의 인덱스 가져오기\n",
    "if word_index is not None:\n",
    "    word_tfidf_values = tfidf_matrix[:, word_index].toarray()  # 해당 단어의 TF-IDF 값 가져오기\n",
    "    print(word_tfidf_values[:5])  # 첫 5개 문서의 TF-IDF 값 출력\n",
    "else:\n",
    "    print(\"단어가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 행렬을 DataFrame으로 변환\n",
    "tfidf_df = pd.DataFrame(dense_matrix, columns=feature_names)\n",
    "\n",
    "# 첫 5개 행 출력\n",
    "tfidf_df\n",
    "tfidf_df.to_csv('책소개_tfidf_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자로 된 토큰이 너무 많아서 비효율적. 숫자 -> 특수토큰 <NUM> 으로 치환하는 전처리 추가.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자를 <NUM>으로 치환\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # 숫자를 <NUM>으로 치환\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나의 공백으로 치환\n",
    "    text = text.lower()  # 영어는 소문자로 변환\n",
    "    text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', text)  # 영어, 한글, <NUM>, 공백을 제외한 모든 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    <num>년간 유머를 경영 리더십에 도입하여 <num>만 명을 웃기고 울린 유머 전...\n",
      "1    마케팅 전문 매거북magabook을 출간하는 모라비안유니타스가 유니타스브랜드 vol...\n",
      "2    애기애타즉 자신을 사랑하고 자신을 사랑하듯 남과 조직을 사랑한다는 것이 바로 도산 ...\n",
      "3    최근 삼국지를 통해서 경영을 바라보려는 시도가 끊임없이 있어왔다ceo의 삼국지는 이...\n",
      "4    경제의 맥락을 짚어주고 경영의 지혜를 알려주는 좋은 책을 찾아낼 수 있도록 돕는 책...\n",
      "Name: processed_intro, dtype: object\n",
      "(4228,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 숫자를 <NUM>으로 치환\n",
    "    text = re.sub(r'\\d+', '<NUM>', str(text))  \n",
    "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나의 공백으로 치환\n",
    "    text = text.lower()  # 영어는 소문자로 변환\n",
    "    text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', str(text))  # 영어, 한글, <NUM>, 공백을 제외한 모든 특수문자 제거\n",
    "    return text\n",
    "\n",
    "# 전체 intro 열에 전처리 함수 적용\n",
    "df['processed_intro'] = df['intro'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 결과 확인\n",
    "print(df['processed_intro'].head())  # 첫 5개 행 출력\n",
    "print(df['processed_intro'].shape)    # 전체 행 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    < num > 년 간 유머 경영 리더십 도입 < num > 만 명 웃기 고 울리다 ...\n",
      "1    마케팅 전문 매 거북 magabook 추다 가다 모라비안 유니 타 스 유니 타 스 ...\n",
      "2    애기 애 타 즉 자신 사랑 하고 자신 사랑 하 듯 남 과 조직 사랑 한다는 것 바로...\n",
      "3    최근 삼국지 통해 서 경영 바라보다 시도 끊임없다 있다 오다 ceo 삼국지 이러하다...\n",
      "4    경제 맥락 짚다 경영 지혜 알다 좋다 책 찾아내다 수 있다 돕다 책 이다 책 학부 ...\n",
      "Name: tokenize_intro, dtype: object\n",
      "(4228,)\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석 및 토큰화\n",
    "def tokenize_text(text):\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text , stem=True)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전체 intro 열에 전처리 함수 적용\n",
    "df['tokenize_intro'] = df['processed_intro'].apply(tokenize_text)\n",
    "\n",
    "# 전처리된 결과 확인\n",
    "print(df['tokenize_intro'].head())  # 첫 5개 행 출력\n",
    "print(df['tokenize_intro'].shape)    # 전체 행 수 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4228, 15848)\n"
     ]
    }
   ],
   "source": [
    "# tf-idf로 벡터화 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "\n",
    "# TF-IDF 행렬의 형태 확인\n",
    "print(tfidf_matrix.shape)  # (문서 수, 단어 수)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4228x15848 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 368650 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abc  academism  acc  acceptable  accountability  ace  act  acting  action  \\\n",
      "0  0.0        0.0  0.0         0.0             0.0  0.0  0.0     0.0     0.0   \n",
      "1  0.0        0.0  0.0         0.0             0.0  0.0  0.0     0.0     0.0   \n",
      "2  0.0        0.0  0.0         0.0             0.0  0.0  0.0     0.0     0.0   \n",
      "3  0.0        0.0  0.0         0.0             0.0  0.0  0.0     0.0     0.0   \n",
      "4  0.0        0.0  0.0         0.0             0.0  0.0  0.0     0.0     0.0   \n",
      "\n",
      "   actively  ...  힘겹다   힘껏  힘드다  힘들다  힘뷰카  힘쓰다  힘없다   힘일  힘주다  힘차다  \n",
      "0       0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1       0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2       0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3       0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4       0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 15848 columns]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 행렬을 밀집 행렬로 변환\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# 단어 목록 추출\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF 행렬을 DataFrame으로 변환\n",
    "tfidf_df = pd.DataFrame(dense_matrix, columns=feature_names)\n",
    "\n",
    "# 첫 5개 행 출력\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.to_csv('책소개_tfidf_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF-IDF 행렬을 DataFrame으로 변환 (sparse matrix를 직접 사용) \n",
    "# # dense 로 하니까 ValueError: Shape of passed values is (4228, 16438), indices imply (4228, 15848) 에러 나서 sparce로 함. \n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # CSV 파일로 저장할 때 청크 단위로 저장\n",
    "# chunk_size = 1000  # 청크 크기 설정\n",
    "# n_chunks = len(tfidf_df_2) // chunk_size + 1  # 총 청크 수 계산\n",
    "\n",
    "# with open(\"책소개_tfidf_2.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     # 헤더 작성 (첫 번째 청크에만)\n",
    "#     f.write(','.join(tfidf_df_2.columns) + '\\n')\n",
    "    \n",
    "#     for i in tqdm(range(n_chunks), desc=\"Saving CSV\"):\n",
    "#         start = i * chunk_size\n",
    "#         end = start + chunk_size\n",
    "#         # 현재 청크에 해당하는 데이터 선택\n",
    "#         chunk = tfidf_df_2.iloc[start:end]\n",
    "#         # 청크를 CSV 파일에 저장 (헤더는 두 번째 청크부터 생략)\n",
    "#         chunk.to_csv(f, header=False, index=False)\n",
    "\n",
    "# print(\"CSV 파일 저장 완료!\")\n",
    "\n",
    "# ##### sparse matrix 로 하니까 저장하는데에 너무 시간이 오래걸림. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 행렬을 DataFrame으로 변환\n",
    "# tfidf_df_2 = pd.DataFrame(dense_matrix, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   title                                          full_name\n",
      "3334         크게 보고 멀리 보라  크게 보고 멀리 보라 초일류 삼성을 일으킨 호암 이병철의 창조적 삶과 경영철학   ...\n",
      "1700         크게 보고 멀리 보라  크게 보고 멀리 보라 초일류 삼성을 일으킨 호암 이병철의 창조적 삶과 경영철학   ...\n",
      "1160             위기를 기회로                          위기를 기회로 호암 탄생 100주년 기념작  \n",
      "3326             위기를 기회로                          위기를 기회로 호암 탄생 100주년 기념작  \n",
      "2707             위기를 기회로                          위기를 기회로 호암 탄생 100주년 기념작  \n",
      "1488          사람중심 기업가정신                사람중심 기업가정신 위기의 한국경제, 사람에게서 희망을 찾다  \n",
      "3522          사람중심 기업가정신                사람중심 기업가정신 위기의 한국경제, 사람에게서 희망을 찾다  \n",
      "3670          사람중심 기업가정신                사람중심 기업가정신 위기의 한국경제, 사람에게서 희망을 찾다  \n",
      "4155          사람중심 기업가정신                사람중심 기업가정신 위기의 한국경제, 사람에게서 희망을 찾다  \n",
      "2991  디지털 시대의 기업가와 기업가정신                               디지털 시대의 기업가와 기업가정신  \n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 계산 \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_recommendations(book_title, df, tfidf_matrix):\n",
    "    # 입력된 책의 인덱스 찾기\n",
    "    idx = df[df['title'] == book_title].index[0]\n",
    "    # 코사인 유사도 계산\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
    "    # 유사도 점수를 기준으로 내림차순 정렬\n",
    "    similar_books_idx = cosine_sim.argsort()[-11:-1][::-1]  # 입력된 책 제외 상위 10권\n",
    "    return df.iloc[similar_books_idx]\n",
    "\n",
    "# 추천 책 리스트 출력\n",
    "recommendations = get_recommendations('크게 보고 멀리 보라', df, tfidf_matrix)\n",
    "print(recommendations[['title', 'full_name']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        한국의 기업 및 기관에게 정보화 바람은 이미 거스를 수 없는 대세. 수많은 기업이 ...\n",
       "1        불확실한 미래, 없어지는 직업 혁신이 가장 필요한 것은 HR 전략이다.세계 최고의 ...\n",
       "2        우리는 i세대를 얼마나 알고 있는가?   관용적이고 평등을 추구하지만 반항적이지 않...\n",
       "3        ‘#소셜쓰고앉았네’는 기업/기관 SNS 담당자, 커뮤니케이터, 마케터를 위한 소셜미...\n",
       "4        우리가 살고 있는 세상은 매우 빠른 속도로 변하고 있다. 특히 기술의 경우에는 우리...\n",
       "                               ...                        \n",
       "40383    “우리가 열심히 버는 건 멋지게 쓰기 위해서이다”_x000D_\\n컴포트슈즈 명가, ...\n",
       "40384    치열한 생존경쟁에서 살아남기 위해서는 힘이 필요하다. 이 책은 독자들에게 힘의 속성...\n",
       "40385    일본 최대의 온라인 쇼핑몰, 라쿠텐 시장 판매자들의 실제 성공 스토리를 담고 있는 ...\n",
       "40386    홍보 주니어를 위한 언론홍보 보도자료 작성 전략, 언론 홍보 베테랑과 현장 기자의 ...\n",
       "40387    홍보업무에 지원하는 학생, 이제 갓 홍보업무를 시작한 홍보주니어. 그들이 가장 알고...\n",
       "Name: intro, Length: 40388, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### 새파일로 돌려보기 \n",
    "data = pd.read_csv('business_economy_books.csv')\n",
    "df = pd.DataFrame(data)\n",
    "df['intro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 숫자를 <NUM>으로 치환\n",
    "    text = re.sub(r'\\d+', '<NUM>', str(text))  \n",
    "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나의 공백으로 치환\n",
    "    text = text.lower()  # 영어는 소문자로 변환\n",
    "    text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', str(text))  # 영어, 한글, <NUM>, 공백을 제외한 모든 특수문자 제거\n",
    "    return text\n",
    "\n",
    "# 형태소 분석 및 토큰화\n",
    "def tokenize_text(text):\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text , stem=True)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "\n",
    "# 코사인 유사도 계산해 책 추천 \n",
    "def get_recommendations(book_title, df, tfidf_matrix):\n",
    "    # 입력된 책의 인덱스 찾기\n",
    "    idx = df[df['title'] == book_title].index[0]\n",
    "    # 코사인 유사도 계산\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
    "    # 유사도 점수를 기준으로 내림차순 정렬\n",
    "    similar_books_idx = cosine_sim.argsort()[-11:-1][::-1]  # 입력된 책 제외 상위 10권\n",
    "    return df.iloc[similar_books_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 intro 열에 전처리 함수 적용\n",
    "df['processed_intro'] = df['intro'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 intro 열에 전처리 함수 적용\n",
    "df['tokenize_intro'] = df['processed_intro'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('business_tokenize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40388, 54479)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "\n",
    "# TF-IDF 행렬의 형태 확인\n",
    "print(tfidf_matrix.shape)  # (문서 수, 단어 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    title                                          full_name\n",
      "13233   레버리지 + 한국의 젊은 부자들                                레버리지 + 한국의 젊은 부자들  \n",
      "37209     포스트 자본주의 새로운 시작                                  포스트 자본주의 새로운 시작  \n",
      "38524       한국은 자본주의 사회인가                                    한국은 자본주의 사회인가  \n",
      "15664      미국주식, 2배로 투자하라                                   미국주식, 2배로 투자하라  \n",
      "21579    세계 역사상 가장 위대한 기회                       세계 역사상 가장 위대한 기회   [ 테이프 1 ]\n",
      "17739               부의 통찰                   부의 통찰 돈의 규칙을 꿰뚫어 찾아낸 5단계 부의 열쇠  \n",
      "37837     필립 코틀러의 다른 자본주의        필립 코틀러의 다른 자본주의 우리 삶이 직면한 위기를 해결하는 14가지 길  \n",
      "15375    무조건 돈이 되는 공부를 하라                                 무조건 돈이 되는 공부를 하라  \n",
      "11658  더 빌리버 THE BELIEVER                               더 빌리버 THE BELIEVER  \n",
      "31855         존리의 부자되기 습관  존리의 부자되기 습관 대한민국 경제독립 액션 플랜   [ 20만부 기념 리커버 에디션 ]\n"
     ]
    }
   ],
   "source": [
    "# 추천 책 리스트 출력\n",
    "recommendations = get_recommendations('레버리지', df, tfidf_matrix)\n",
    "print(recommendations[['title', 'full_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              full_name\n",
      "30109       EBS 다큐프라임 자본주의 + 자본주의 사용설명서   [ 특별구성, 전2권 ]\n",
      "30124                            EBS 다큐프라임 자본주의 사용설명서  \n",
      "30110            자본주의 + 필립 코틀러의 다른 자본주의   [ 특별구성, 전2권 ]\n",
      "30103                              자본의 역습 경제학적 상상과 비판  \n",
      "37209                                 포스트 자본주의 새로운 시작  \n",
      "30144                                   자본주의를 다시 생각한다  \n",
      "38524                                   한국은 자본주의 사회인가  \n",
      "37837       필립 코틀러의 다른 자본주의 우리 삶이 직면한 위기를 해결하는 14가지 길  \n",
      "30158  자본주의자들의 바이블 자본가들이 사실은 말하고 싶지 않은 리얼 자본주의   [ 양장 ]\n",
      "14784                      메뚜기와 꿀벌 약탈과 창조, 자본주의의 두 얼굴  \n"
     ]
    }
   ],
   "source": [
    "# 추천 책 리스트 출력\n",
    "recommendations = get_recommendations('자본주의', df, tfidf_matrix)\n",
    "print(recommendations[['full_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ 책 여러권 입력 \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt  # 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        한국 기업 및 기관 에게 정보화 바람 이미 거스 수 없다 대세 수많다 기업 이미 e...\n",
       "1        불확실하다 미래 없어지다 직업 혁신 가장 필요하다 것 hr 전략 이다 세계 최고 회...\n",
       "2        우리 i 세대 얼마나 알 고 있다 관용 적 이고 평등 추구 하 지만 반 항적 이지 ...\n",
       "3        소셜 쓰다 앉다 늘다 기업 기관 sns 담당자 커뮤 니 케이 터 마케터 위 한 소셜...\n",
       "4        우리 살 고 있다 세상 매우 빠르다 속도 로 변하다 있다 특히 기술 경우 에는 우리...\n",
       "                               ...                        \n",
       "40383    우리 열심히 버 건 멋지다 쓰기 위 x < num > d 컴 포트 슈즈 명가 주바 ...\n",
       "40384    치열하다 생존 경쟁 에서 살아나다 위 힘 필요하다 책 독자 들 에게 힘 속성 대해 ...\n",
       "40385    일본 최대 온라인 쇼핑몰 라쿠텐 시장 판매 자 들 실제 성공 스토리 담다 있다 책 ...\n",
       "40386    홍보 주니어 위 한 언론 홍보 보도자료 작성 전략 언론 홍보 베테 랑 과 현장 기자...\n",
       "40387    홍보 업무 지원 학생 이제 갓 홍보 업무 시작 한 홍보 주니어 들 가장 알 고 싶다...\n",
       "Name: tokenize_intro, Length: 40388, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('business_tokenize.csv')\n",
    "df = pd.DataFrame(file)\n",
    "df['tokenize_intro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30109"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df[df['title'] == '자본주의 + 자본주의 사용설명서'].index[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자본주의 + 자본주의 사용설명서'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][30109]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ 책 여러권 입력 \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt  # 형태소 분석기\n",
    "\n",
    "# 전처리 함수 정의\n",
    "# def preprocess_text(text):\n",
    "#     text = re.sub(r'\\d+', '<NUM>', str(text))  \n",
    "#     text = re.sub(r'\\s+', ' ', text)  \n",
    "#     text = text.lower()  \n",
    "#     text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', str(text))  \n",
    "#     return text\n",
    "\n",
    "# # 형태소 분석 및 토큰화\n",
    "# def tokenize_text(text):\n",
    "#     okt = Okt()\n",
    "#     tokens = okt.morphs(text, stem=True)\n",
    "#     stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "#     tokens = [token for token in tokens if token not in stopwords]\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# # TF-IDF 벡터화 함수\n",
    "# def vectorize_text(df, text_column):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "#     return tfidf_matrix, vectorizer\n",
    "\n",
    "# TF-IDF 벡터화 함수\n",
    "def vectorize_text(df, text_column):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# 코사인 유사도 계산 및 추천 함수\n",
    "def get_recommendations(book_titles, df):\n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix, vectorizer = vectorize_text(df, 'tokenize_intro')\n",
    "\n",
    "    # 입력된 책 제목들의 인덱스 찾기\n",
    "    indices = df[df['title'].isin(book_titles)].index.tolist()\n",
    "    \n",
    "    # 추천 리스트 초기화\n",
    "    recommendations = {}\n",
    "\n",
    "    for idx in indices:\n",
    "        # 현재 책의 코사인 유사도 계산\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
    "        # 유사도 점수를 기준으로 내림차순 정렬\n",
    "        similar_books_idx = cosine_sim.argsort()[-11:-1][::-1]  # 자기 자신 제외 상위 10권\n",
    "        \n",
    "        for i in similar_books_idx:\n",
    "            book_title = df.iloc[i]['title']\n",
    "            score = cosine_sim[i]\n",
    "            if book_title not in recommendations:\n",
    "                recommendations[book_title] = score\n",
    "            else:\n",
    "                recommendations[book_title] = max(recommendations[book_title], score)  # 유사도 점수 갱신\n",
    "\n",
    "    # 유사도 점수 기준으로 추천 리스트 정렬\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 상위 10권 반환\n",
    "    return sorted_recommendations[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 책: 지금 중국 주식 천만원이면 10년 후 강남 아파트를 산다 + 5년 후 포르쉐 타고 싶다면 미국 주식 지금 당장 올라타라, 유사도 점수: 0.4526\n",
      "추천 책: 5년 후 포르쉐 타고 싶다면 미국 주식 지금 당장 올라타라, 유사도 점수: 0.4247\n",
      "추천 책: 주식 투자로 1,000만 원에서 100억 원 만들기 플랜, 유사도 점수: 0.4228\n",
      "추천 책: 국내외 기계 산업 시장 분석과 비즈니스 전략 (상), 유사도 점수: 0.4128\n",
      "추천 책: 국내외 기계 산업 시장 분석과 비즈니스 전략 (하), 유사도 점수: 0.4128\n",
      "추천 책: 2024 탄소중립 이행을 위한 신재생에너지/에너지 및 자원재활용 시장분석과 기술개발 전략(하), 유사도 점수: 0.4121\n",
      "추천 책: 2024 탄소중립 이행을 위한 신재생에너지/에너지 및 자원재활용 시장분석과 기술개발 전략(상), 유사도 점수: 0.4119\n",
      "추천 책: 지금 시작해도 주식투자는 복리다, 유사도 점수: 0.4116\n",
      "추천 책: 2024 국내외 디스플레이 시장 및 기술 분석과 해외 진출 전략(하), 유사도 점수: 0.4026\n",
      "추천 책: 2024 국내외 디스플레이 시장 및 기술 분석과 해외 진출 전략(상), 유사도 점수: 0.4026\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시: \n",
    "book_titles = ['문명, 현대화 그리고 가치투자와 중국']  # 사용자가 입력한 책 제목들\n",
    "recommended_books = get_recommendations(book_titles, df)\n",
    "\n",
    "# 추천 결과 출력\n",
    "for title, score in recommended_books:\n",
    "    print(f\"추천 책: {title}, 유사도 점수: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m recommended_books \u001b[38;5;241m=\u001b[39m get_recommendations(book_titles, df)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 추천 결과 출력\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, score \u001b[38;5;129;01min\u001b[39;00m recommended_books:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m추천 책: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 유사도 점수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 사용 예시:평균으로취합시결과_오류발생\n",
    "book_titles = ['문명, 현대화 그리고 가치투자와 중국']  # 사용자가 입력한 책 제목들\n",
    "recommended_books = get_recommendations(book_titles, df)\n",
    "\n",
    "# 추천 결과 출력\n",
    "for title, score in recommended_books:\n",
    "    print(f\"추천 책: {title}, 유사도 점수: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 책: 회계 천재가 된 홍 대리 1, 유사도 점수: 0.9282\n",
      "추천 책: 회계 천재가 된 홍 대리 5, 유사도 점수: 0.9276\n",
      "추천 책: 회계 천재가 된 홍 대리 3, 유사도 점수: 0.9271\n",
      "추천 책: 회계 천재가 된 홍 대리 4, 유사도 점수: 0.9015\n",
      "추천 책: 회계 천재가 된 홍대리 세트 (전5권), 유사도 점수: 0.7855\n",
      "추천 책: 회계 천재가 된 홍대리 1, 유사도 점수: 0.7762\n",
      "추천 책: 회계 천재가 된 홍대리 2, 유사도 점수: 0.7725\n",
      "추천 책: 회계 천재가 된 홍대리 3, 유사도 점수: 0.7532\n",
      "추천 책: 부의 시나리오 + 부의 대이동, 유사도 점수: 0.7517\n",
      "추천 책: 회계 천재가 된 홍 대리 1~5 세트, 유사도 점수: 0.7133\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "book_titles = ['회계 천재가 된 홍 대리 2', '부의 시나리오']  # 사용자가 입력한 책 제목들\n",
    "recommended_books = get_recommendations(book_titles, df)\n",
    "\n",
    "# 추천 결과 출력\n",
    "for title, score in recommended_books:\n",
    "    print(f\"추천 책: {title}, 유사도 점수: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 책: 부의 시나리오 + 부의 대이동, 유사도 점수: 0.7517\n",
      "추천 책: [예스리커버] 오건영 30만 부 기념 에디션 세트, 유사도 점수: 0.6517\n",
      "추천 책: 인플레이션에서 살아남기, 유사도 점수: 0.2104\n",
      "추천 책: 국제금융지식이 돈이다, 유사도 점수: 0.2093\n",
      "추천 책: 새로운 금융시장, 유사도 점수: 0.2090\n",
      "추천 책: 인플레로 돈버는 사람들, 유사도 점수: 0.1983\n",
      "추천 책: 손님이 꼬리에 꼬리를 무는 가게 만들기, 유사도 점수: 0.1797\n",
      "추천 책: 금융을 모르는 그대에게, 유사도 점수: 0.1794\n",
      "추천 책: 금융시장의 이해, 유사도 점수: 0.1720\n",
      "추천 책: 돈의 감각을 길러주는 경제 지식 첫 걸음, 유사도 점수: 0.1710\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "book_titles = ['부의 시나리오']  # 사용자가 입력한 책 제목들\n",
    "recommended_books = get_recommendations(book_titles, df)\n",
    "\n",
    "# 추천 결과 출력\n",
    "for title, score in recommended_books:\n",
    "    print(f\"추천 책: {title}, 유사도 점수: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# # 전처리 함수 정의\n",
    "# def preprocess_text(text):\n",
    "#     # 숫자를 <NUM>으로 치환\n",
    "#     text = re.sub(r'\\d+', '<NUM>', str(text))  \n",
    "#     text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나의 공백으로 치환\n",
    "#     text = text.lower()  # 영어는 소문자로 변환\n",
    "#     text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', str(text))  # 영어, 한글, <NUM>, 공백을 제외한 모든 특수문자 제거\n",
    "#     return text\n",
    "\n",
    "# # 형태소 분석 및 토큰화\n",
    "# def tokenize_text(text):\n",
    "#     okt = Okt()\n",
    "#     tokens = okt.morphs(text, stem=True)\n",
    "\n",
    "#     # 불용어 제거\n",
    "#     stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "#     tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# TF-IDF 벡터화 및 평균 코사인 유사도 계산\n",
    "def get_recommendations(book_titles, df):\n",
    "    # 전처리 및 토큰화\n",
    "    # df['processed_intro'] = df['intro'].apply(preprocess_text)\n",
    "    # df['tokenized_intro'] = df['processed_intro'].apply(tokenize_text)\n",
    "\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "\n",
    "    # 여러 권의 책에 대한 평균 코사인 유사도 계산\n",
    "    cosine_similarities = []\n",
    "    for book_title in book_titles:\n",
    "        idx = df[df['title'] == book_title].index[0]\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "\n",
    "    # 평균 코사인 유사도 계산\n",
    "    avg_cosine_sim = sum(cosine_similarities) / len(cosine_similarities)\n",
    "\n",
    "    # 유사도 점수를 기준으로 내림차순 정렬\n",
    "    similar_books_idx = avg_cosine_sim.argsort()[-11:-1][::-1]  # 입력된 책 제외 상위 10권\n",
    "    return df.iloc[similar_books_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m recommended_books \u001b[38;5;241m=\u001b[39m get_recommendations(book_titles, df)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 추천 결과 출력\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, score \u001b[38;5;129;01min\u001b[39;00m recommended_books:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m추천 책: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 유사도 점수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "book_titles = ['부의 시나리오']  # 사용자가 입력한 책 제목들\n",
    "recommended_books = get_recommendations(book_titles, df)\n",
    "\n",
    "# 추천 결과 출력\n",
    "for title, score in recommended_books:\n",
    "    print(f\"추천 책: {title}, 유사도 점수: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          title\n",
      "17797              부자 아빠 가난한 아빠\n",
      "17694          부의 시나리오 + 부의 대이동\n",
      "17817             부자 아빠의 투자 가이드\n",
      "17801          부자 아빠 가난한 아빠 SET\n",
      "25134      앞으로 10년, 돈의 배반이 시작된다\n",
      "17808         부자 아빠의 21세기형 비즈니스\n",
      "24585                   아빠를 팝니다\n",
      "18015                부자아빠의 진실게임\n",
      "17800  부자 아빠 가난한 아빠 20주년 특별 기념판\n",
      "18070                    부자의 언어\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "input_books = [\"부자 아빠 가난한 아빠\", \"부의 시나리오\"]  # 사용자가 입력한 책 제목 리스트\n",
    "recommended_books = get_recommendations(input_books, df)\n",
    "print(recommended_books[['title']])  # 추천된 책 제목과 소개글 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN으로 취합 \n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# # 전처리 함수 정의\n",
    "# def preprocess_text(text):\n",
    "#     text = re.sub(r'\\d+', '<NUM>', str(text))  \n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', str(text))  \n",
    "#     return text\n",
    "\n",
    "# # 형태소 분석 및 토큰화\n",
    "# def tokenize_text(text):\n",
    "#     okt = Okt()\n",
    "#     tokens = okt.morphs(text, stem=True)\n",
    "\n",
    "#     # 불용어 제거\n",
    "#     stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '하다', '또한', '그', '그리고', '하지만']\n",
    "#     tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# KNN 추천 시스템 함수\n",
    "def knn_recommendations(book_titles, df, k=10):\n",
    "    # 전처리 및 토큰화\n",
    "    # df['processed_intro'] = df['intro'].apply(preprocess_text)\n",
    "    # df['tokenized_intro'] = df['processed_intro'].apply(tokenize_text)\n",
    "\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "\n",
    "    # KNN 모델 학습\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    knn.fit(tfidf_matrix)\n",
    "\n",
    "    # 여러 권의 책에 대한 유사도 계산\n",
    "    input_vectors = []\n",
    "    for book_title in book_titles:\n",
    "        # 입력된 책 제목으로 인덱스 찾기\n",
    "        idx = df[df['title'] == book_title].index[0]\n",
    "        input_vectors.append(tfidf_matrix[idx])\n",
    "\n",
    "    # 평균 벡터 계산\n",
    "    input_vector_mean = sum(input_vectors) / len(input_vectors)\n",
    "\n",
    "    # KNN을 이용해 가장 유사한 책 찾기\n",
    "    distances, indices = knn.kneighbors(input_vector_mean.reshape(1, -1))\n",
    "\n",
    "    # 추천된 책 데이터 반환\n",
    "    recommended_books = df.iloc[indices.flatten()]\n",
    "    return recommended_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  title\n",
      "39962   회계 천재가 된 홍 대리 2\n",
      "17693           부의 시나리오\n",
      "39965   회계 천재가 된 홍 대리 5\n",
      "39963   회계 천재가 된 홍 대리 3\n",
      "39961   회계 천재가 된 홍 대리 1\n",
      "39964   회계 천재가 된 홍 대리 4\n",
      "17694  부의 시나리오 + 부의 대이동\n",
      "39968    회계 천재가 된 홍대리 2\n",
      "39967    회계 천재가 된 홍대리 1\n",
      "39969    회계 천재가 된 홍대리 3\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "input_books = [\"회계 천재가 된 홍 대리 2\", \"부의 시나리오\"]  \n",
    "\n",
    "recommended_books = knn_recommendations(input_books, df)\n",
    "print(recommended_books[['title']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 입력한 자기자신 제외하고 KNN \n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # 전처리 함수 정의\n",
    "# def preprocess_text(text):\n",
    "#     text = re.sub(r'\\d+', '<NUM>', str(text))  \n",
    "#     text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나의 공백으로 치환\n",
    "#     text = text.lower()  # 영어는 소문자로 변환\n",
    "#     text = re.sub(r'[^a-z가-힣<NUM>\\s]', '', str(text))  # 영어, 한글, <NUM>, 공백을 제외한 모든 특수문자 제거\n",
    "#     return text\n",
    "\n",
    "# # TF-IDF 벡터화 및 KNN 모델 생성\n",
    "# def create_knn_model(df):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "    \n",
    "#     knn = NearestNeighbors(n_neighbors=11, metric='cosine')  # 입력한 책을 제외하기 위해 11개 찾기\n",
    "#     knn.fit(tfidf_matrix)\n",
    "#     return knn, vectorizer, tfidf_matrix\n",
    "\n",
    "# # 책 추천 함수\n",
    "# def get_recommendations(input_books, df, knn, vectorizer, tfidf_matrix):\n",
    "#     # 입력된 책들을 전처리 및 벡터화\n",
    "#     input_texts = [preprocess_text(book) for book in input_books]\n",
    "#     input_vectors = vectorizer.transform(input_texts)\n",
    "    \n",
    "#     # 평균 벡터 계산\n",
    "#     input_vector_mean = input_vectors.mean(axis=0)\n",
    "    \n",
    "#     # KNN을 통해 가장 가까운 책 찾기\n",
    "#     distances, indices = knn.kneighbors(input_vector_mean, n_neighbors=11)\n",
    "    \n",
    "#     # 입력된 책 제외\n",
    "#     similar_books_idx = [idx for idx in indices.flatten() if df.iloc[idx]['title'] not in input_books]\n",
    "    \n",
    "#     return df.iloc[similar_books_idx][:10]  # 상위 10권 반환\n",
    "\n",
    "# # 데이터프레임 생성 및 모델 생성\n",
    "# df = pd.DataFrame({'title': ['책1', '책2', '책3', '책4'], 'intro': ['소개1', '소개2', '소개3', '소개4']})\n",
    "# knn_model, vectorizer, tfidf_matrix = create_knn_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 책 추천\n",
    "input_books = [\"회계 천재가 된 홍 대리 2\", \"부의 시나리오\"]  # 사용자가 입력한 책\n",
    "recommended_books = get_recommendations(input_books, df, knn_model, vectorizer, tfidf_matrix)\n",
    "\n",
    "print(recommended_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 책1에 대한 유사도 , 책 2에 대한 유사도 , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 새파일로 돌려보기 \n",
    "data = pd.read_csv('final_book3_token.csv')\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_recommendations(book_titles, df):\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['tokenize_intro'])\n",
    "\n",
    "    # 여러 권의 책에 대한 평균 코사인 유사도 계산\n",
    "    cosine_similarities = []\n",
    "    for book_title in book_titles:\n",
    "        idx = df[df['title'] == book_title].index[0]\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "\n",
    "    # 평균 코사인 유사도 계산\n",
    "    avg_cosine_sim = sum(cosine_similarities) / len(cosine_similarities)\n",
    "\n",
    "    # 유사도 점수를 기준으로 내림차순 정렬\n",
    "    similar_books_idx = avg_cosine_sim.argsort()[-11:-1][::-1]  # 입력된 책 제외 상위 10권\n",
    "\n",
    "    # 추천 결과와 유사도 점수를 함께 반환\n",
    "    recommended_books = df.iloc[similar_books_idx].copy()\n",
    "    recommended_books['similarity'] = avg_cosine_sim[similar_books_idx]\n",
    "\n",
    "    return recommended_books[['title', 'similarity']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   title  similarity\n",
      "6082                              퀀트의 세계    0.462932\n",
      "6019                      퀀트 투자 무작정 따라하기    0.439547\n",
      "5096                              퀀트의 정석    0.401012\n",
      "11776  더퍼슨스(the Persons) No.1: 퀀트(Quant)    0.359308\n",
      "41173                         퀀트 30년의 기록    0.343942\n",
      "19604                      R로 하는 퀀트 트레이딩    0.343145\n",
      "20040        인공지능 투자가 퀀트 + 할 수 있다! 퀀트 투자    0.332329\n",
      "19881                      할 수 있다! 퀀트 투자    0.325740\n",
      "15194                         퀀트로 가치투자하라    0.324720\n",
      "9181                        하면 된다! 퀀트 투자    0.318868\n"
     ]
    }
   ],
   "source": [
    "# 추천할 책 제목 입력\n",
    "book_titles = ['돈의 물리학']\n",
    "\n",
    "# 추천 결과 가져오기\n",
    "recommendations = get_recommendations(book_titles, df)\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   title  similarity\n",
      "11561                             돈의 물리학    0.731466\n",
      "6019                      퀀트 투자 무작정 따라하기    0.579031\n",
      "5096                              퀀트의 정석    0.526845\n",
      "11776  더퍼슨스(the Persons) No.1: 퀀트(Quant)    0.489053\n",
      "19604                      R로 하는 퀀트 트레이딩    0.456720\n",
      "20040        인공지능 투자가 퀀트 + 할 수 있다! 퀀트 투자    0.439011\n",
      "41173                         퀀트 30년의 기록    0.437844\n",
      "19881                      할 수 있다! 퀀트 투자    0.426186\n",
      "19564                SMART BETA (스마트 베타)    0.410443\n",
      "9181                        하면 된다! 퀀트 투자    0.410198\n"
     ]
    }
   ],
   "source": [
    "# 추천할 책 제목 입력\n",
    "book_titles = ['퀀트의 세계','돈의 물리학']\n",
    "\n",
    "# 추천 결과 가져오기\n",
    "recommendations = get_recommendations(book_titles, df)\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      title  similarity\n",
      "42109              새로운 금융시장    0.300723\n",
      "52173           국제금융지식이 돈이다    0.243228\n",
      "28141     경제학이 풀지 못한 시장의 비밀    0.236477\n",
      "39666              금융시장의 이해    0.218244\n",
      "49014             금융 시장의 이해    0.214529\n",
      "15253          금융을 모르는 그대에게    0.213056\n",
      "36464                자본의 전략    0.205292\n",
      "8369      주식투자,거인의 어깨에 올라타라    0.202824\n",
      "42661  금융시장을 지배하는 핵심 키워드 83    0.202195\n",
      "28283          투자와 비이성적 마인드    0.202089\n"
     ]
    }
   ],
   "source": [
    "# 추천할 책 제목 입력\n",
    "book_titles = ['금융시장으로 간 진화론']\n",
    "\n",
    "# 추천 결과 가져오기\n",
    "recommendations = get_recommendations(book_titles, df)\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       title  similarity\n",
      "2844         회계 천재가 된 홍 대리 2    0.516454\n",
      "2852         회계 천재가 된 홍 대리 5    0.481379\n",
      "2849         회계 천재가 된 홍 대리 3    0.479583\n",
      "2841         회계 천재가 된 홍 대리 1    0.479328\n",
      "2837         회계 천재가 된 홍 대리 4    0.466111\n",
      "18638  회계 천재가 된 홍대리 세트 (전5권)    0.403880\n",
      "18641         회계 천재가 된 홍대리 2    0.403390\n",
      "9956        부의 시나리오 + 부의 대이동    0.402823\n",
      "18640         회계 천재가 된 홍대리 1    0.402094\n",
      "18639         회계 천재가 된 홍대리 3    0.394713\n"
     ]
    }
   ],
   "source": [
    "# 추천할 책 제목 입력\n",
    "book_titles = ['회계 천재가 된 홍 대리 2', '부의 시나리오']\n",
    "\n",
    "# 추천 결과 가져오기\n",
    "recommendations = get_recommendations(book_titles, df)\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr 추가 , fasttext 랑 8:2 로 넣기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_mmr_recommendations(book_title, cosine_sim, df, alpha=0.5, num_recommendations=5):\n",
    "    # 책 제목에 대한 인덱스 가져오기\n",
    "    idx = df[df['title'] == book_title].index[0]\n",
    "\n",
    "    # 관련성 점수 (코사인 유사도)\n",
    "    scores = cosine_sim[idx]\n",
    "\n",
    "    # MMR을 위한 초기 후보 목록 생성\n",
    "    recommended_indices = set()\n",
    "    ranked_indices = np.argsort(scores)[::-1]  # 내림차순 정렬\n",
    "\n",
    "    for i in ranked_indices:\n",
    "        if len(recommended_indices) >= num_recommendations:\n",
    "            break\n",
    "        \n",
    "        if i not in recommended_indices:\n",
    "            # MMR 계산\n",
    "            relevance_score = scores[i]\n",
    "            diversity_score = sum(cosine_sim[i][j] for j in recommended_indices)\n",
    "            mmr_score = alpha * relevance_score - (1 - alpha) * diversity_score\n",
    "\n",
    "            # MMR 점수가 높은 항목 선택\n",
    "            if mmr_score > 0:\n",
    "                recommended_indices.add(i)\n",
    "\n",
    "    # 추천 결과 반환\n",
    "    recommended_books = df.iloc[list(recommended_indices)].copy()\n",
    "    recommended_books['similarity'] = [scores[i] for i in recommended_indices]\n",
    "\n",
    "    return recommended_books[['title', 'similarity']]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 이미 계산된 코사인 유사도 행렬 (여기서는 예시로 랜덤 생성)\n",
    "cosine_sim = np.array([\n",
    "    [1.0, 0.8, 0.3, 0.2, 0.4, 0.1],\n",
    "    [0.8, 1.0, 0.5, 0.1, 0.3, 0.2],\n",
    "    [0.3, 0.5, 1.0, 0.6, 0.2, 0.4],\n",
    "    [0.2, 0.1, 0.6, 1.0, 0.3, 0.5],\n",
    "    [0.4, 0.3, 0.2, 0.3, 1.0, 0.7],\n",
    "    [0.1, 0.2, 0.4, 0.5, 0.7, 1.0]\n",
    "])\n",
    "\n",
    "# 추천할 책 제목\n",
    "book_title = '책1'\n",
    "\n",
    "# 추천 결과 가져오기\n",
    "recommendations = get_mmr_recommendations(book_title, cosine_sim, df)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
