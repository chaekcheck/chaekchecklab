{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_path = r\"D:\\python_project\\chaekchecklab\\data\\basic\"\n",
    "detail_path = r\"D:\\python_project\\chaekchecklab\\data\\detail\"\n",
    "\n",
    "basic_list = os.listdir(basic_path)\n",
    "detail_list = os.listdir(detail_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detail_info(book_url):\n",
    "    res = requests.get(book_url, headers= headers)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    # 작가\n",
    "    author_list = []\n",
    "\n",
    "    author_links = [e['href'] for e in soup.select('span.moreAuthLiCont>ul>li>a')]\n",
    "    if not author_links:\n",
    "        author_links = [e['href'] for e in soup.select('span.gd_auth>a')]\n",
    "\n",
    "    for author_link in author_links:\n",
    "        author_name = re.search(r'author=(.+)$', author_link).group(1)\n",
    "        if re.search(r'%..', author_name):\n",
    "            author_name = urllib.parse.unquote(author_name).replace('+', ' ')\n",
    "        try:\n",
    "            author_code = re.search(r'authorNo=(\\d+)', author_link).group(1)\n",
    "            author_list.append([author_name, author_code])\n",
    "        except AttributeError:\n",
    "            # author_code = ''\n",
    "            author_list.append([author_name])\n",
    "    \n",
    "    if not author_links:\n",
    "        author_list = [e.text for e in soup.select('span.moreAuthLiCont>ul>li')]\n",
    "        if not author_links:\n",
    "            author_list = [e.text.strip() for e in soup.select('span.gd_auth')]\n",
    "    # print(author_list)s\n",
    "\n",
    "\n",
    "    # 품목 정보\n",
    "    item_box = soup.select_one('tbody.b_size')\n",
    "    item_infos = [e.text for e in item_box.select('td')]\n",
    "    # print(item_infos)\n",
    "\n",
    "    # Category\n",
    "    cates = [e.text.strip().replace('\\n>\\n', ' > ') for e in soup.select('div#infoset_goodsCate li')]\n",
    "    # print(cates)\n",
    "\n",
    "    # 책 소개\n",
    "    try:\n",
    "        intro = soup.select_one('div.infoWrap_txtInner').text.strip()\n",
    "    except AttributeError:\n",
    "        intro = \"\"\n",
    "    # print(intro)\n",
    "    \n",
    "    # break\n",
    "    return [book_url, author_list, str(item_infos), str(cates), intro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_infos(b, d):\n",
    "    b_path = os.path.join(basic_path, b)\n",
    "    d_path = os.path.join(detail_path, d)\n",
    "    # print(b_path, d_path)\n",
    "\n",
    "    # print(f\"Read csv files...\")\n",
    "    basic_urls = pd.read_csv(b_path).book_url.to_list()\n",
    "    detail_urls = pd.read_csv(d_path).book_url.to_list()\n",
    "\n",
    "    remain_list = set(basic_urls) - set(detail_urls)\n",
    "    # print(f\"{len(remain_list)}개 추가해야함\")\n",
    "\n",
    "    # print(f\"Get informations\")\n",
    "    added_list = []\n",
    "    for book_url in remain_list:\n",
    "        added_list.append(get_detail_info(book_url))\n",
    "\n",
    "    added_df = pd.DataFrame(added_list)\n",
    "    # print(f\"Add {len(added_df)} rows!\")\n",
    "    \n",
    "    added_df.to_csv(d_path, index=False, mode='a', header=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    # print(f\"Have Saved {len(added_df)} rows!\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [04:12<00:00,  6.15s/it]  \n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(fill_infos, basic_list, detail_list), total=len(basic_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, d in zip(basic_list, detail_list):\n",
    "    b_path = os.path.join(basic_path, b)\n",
    "    d_path = os.path.join(detail_path, d)\n",
    "    # print(b_path, d_path)\n",
    "\n",
    "    # print(f\"Read csv files...\")\n",
    "    basic_urls = pd.read_csv(b_path).book_url.to_list()\n",
    "    detail_urls = pd.read_csv(d_path).book_url.to_list()\n",
    "\n",
    "    print(len(basic_urls) == len(detail_urls), len(basic_urls), len(detail_urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
